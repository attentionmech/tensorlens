{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attentionmech/tensorlens/blob/main/tensorlens/notebooks/gpt2_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d8hkkmoqx4NB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da5a080-772f-4c19-e34d-afac2aa0bb0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2KThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "!pip install -q uv\n",
        "!uv run --prerelease=allow https://raw.githubusercontent.com/attentionmech/smolbox/refs/heads/main/smolbox/tools/inspect/tensorlens_activations.py run --notebook True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fire\n",
        "import torch\n",
        "from smolbox.core.state_manager import AUTORESOLVE, resolve\n",
        "from smolbox.core.tool_manager import BaseTool\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from tensorlens.tensorlens import trace, viewer\n",
        "\n",
        "class TensorLensActivations(BaseTool):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path=AUTORESOLVE,\n",
        "        prompt=\"Once upon a time \"*10,\n",
        "        max_new_tokens=1,  # Generate only 1 new token\n",
        "        host=\"localhost\",\n",
        "        port=8000,\n",
        "        notebook=False,\n",
        "    ):\n",
        "        self.model_path = resolve(\"model_path\", model_path)\n",
        "        self.text_input = prompt\n",
        "        self.max_new_tokens = int(max_new_tokens)\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.notebook = notebook\n",
        "\n",
        "    def run(self):\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(self.model_path)\n",
        "        model.eval()\n",
        "\n",
        "        # Set padding token if it's None\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        if model.config.pad_token_id is None:\n",
        "            model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "        # Tokenize the input text\n",
        "        input_ids = tokenizer(\n",
        "            self.text_input or \"Once upon a time\"*10,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        # Step 1: Perform a forward pass to get activations for the prompt\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
        "\n",
        "        # Trace activations for the prompt tokens (only)\n",
        "        for step_idx, layer_outputs in enumerate(outputs.hidden_states):\n",
        "            for layer_idx, hidden in enumerate(layer_outputs):\n",
        "                trace(f\"layer_{layer_idx}_step_{step_idx}\", hidden.detach().cpu().numpy())\n",
        "\n",
        "        # Step 2: Generate 1 new token based on the prompt\n",
        "        with torch.no_grad():\n",
        "            outputs_gen = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_new_tokens=self.max_new_tokens,  # Only generate 1 new token\n",
        "                return_dict_in_generate=True,\n",
        "                output_hidden_states=True,\n",
        "                output_scores=False,\n",
        "                output_attentions=False,\n",
        "                do_sample=False,  # Greedy decoding\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        # Step 3: Trace activations for the first generated token\n",
        "        for step_idx, layer_outputs in enumerate(outputs_gen.hidden_states):\n",
        "            for layer_idx, hidden in enumerate(layer_outputs):\n",
        "                trace(f\"layer_{layer_idx}_step_gen_{step_idx}\", hidden.detach().cpu().numpy())\n",
        "\n",
        "        viewer(height=\"600\",width='100%', port=self.port, host=self.host, notebook=self.notebook)\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "ab6oUHoj5TQD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TensorLensActivations(notebook=True).run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "Lu56CuQu5ZrI",
        "outputId": "3cf429ea-089b-4c80-f8b2-38a346a50ee0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(8000, {'cache': false}));\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '600');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGSoeH5n5uF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyVJP4t7mTQQZdyY3yR3Sq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}